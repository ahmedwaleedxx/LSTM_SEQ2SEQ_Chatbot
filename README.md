<!--
**ahmedwaleedxx/ahmedwaleedxx** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
<h1 align="center">LSTM Seq2Seq Chatbot Model</h1>
<h3 align="center">A Chatbot model that is focusing on Tourism in Egypt</h3>
 
<img align="right" alt="coding" width="400" src="Sample.gif">
<!-- <p align="left"> <img src="https://komarev.com/ghpvc/?username=ahmedwaleedx&label=Profile%20views&color=0e75b6&style=flat" alt="ahmedwaleedx" /> </p>
 -->
<!-- <p align="left"> <a href="https://twitter.com/ahmedwaleedxx" target="blank"><img src="https://img.shields.io/twitter/follow/ahmedwaleedxx?logo=twitter&style=for-the-badge" alt="ahmedwaleedxx" /></a> </p> -->

- ðŸŒ± This model consists of 
- **Encoder**: 
 >   It takes the input sequence and encodes it into a fixed-length vector, which is known as the "context vector" or "thought vector." The encoder is typically composed of a stack of LSTM cells. Each LSTM cell takes an input element from the sequence and updates its internal hidden state and cell state based on the current input and the previous hidden state. This allows the LSTM to capture long-term dependencies and encode the input sequence into a condensed representation.
- **Decoder**:
 >   The decoder takes the context vector generated by the encoder and generates the output sequence one element at a time. Like the encoder, the decoder is also composed of a stack of LSTM cells. However, it has an additional input at each time step, which is the previously generated element of the output sequence. This input helps the decoder to condition its predictions on the previously generated elements and generate a meaningful output sequence.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>  During training, the model is trained to minimize the difference between the predicted output sequence and the target output sequence. This is typically done using a loss function like the cross-entropy loss. The encoder and decoder are jointly trained using backpropagation through time, which propagates the gradients from the decoder to the encoder, allowing the model to learn to generate meaningful and coherent output sequences.

>>  During inference or testing, the model is fed with an input sequence through the encoder, and then the decoder generates the output sequence element by element by recursively feeding the previously generated elements as inputs. This process continues until an end-of-sequence token is generated, or a predefined maximum length is reached.

>>  The LSTM Seq2Seq model has been proven to be effective in various sequence-to-sequence tasks and has been widely adopted due to its ability to handle variable-length input and output sequences and capture long-term dependencies.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

- ðŸ“« Datasets that were used are included in the project files

<h3 align="left">Connect with me:</h3>
<p align="left">
<a href="https://twitter.com/ahmedwaleedxx" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/twitter.svg" alt="ahmedwaleedxx" height="30" width="40" /></a>
<a href="https://linkedin.com/in/ahmedwaleedx" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg" alt="ahmedwaleedx" height="30" width="40" /></a>
<a href="https://kaggle.com/ahmedwaleedxx" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/kaggle.svg" alt="ahmedwaleedxx" height="30" width="40" /></a>
<a href="https://fb.com/ahmedwaleedx" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/facebook.svg" alt="ahmedwaleedx" height="30" width="40" /></a>
<a href="https://instagram.com/ahmedwaleedx" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/instagram.svg" alt="ahmedwaleedx" height="30" width="40" /></a>
</p>

<h3 align="left">Languages and Tools that were used:</h3>
<a href="https://www.python.org" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a> 
<a href="https://www.tensorflow.org" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/tensorflow/tensorflow-icon.svg" alt="tensorflow" width="40" height="40"/> </a> 
<!-- <p><img align="left" src="https://github-readme-stats.vercel.app/api/top-langs?username=ahmedwaleedx&show_icons=true&locale=en&layout=compact" alt="ahmedwaleedx" /></p>
 
<p>&nbsp;<img align="center" src="https://github-readme-stats.vercel.app/api?username=ahmedwaleedx&show_icons=true&locale=en" alt="ahmedwaleedx" /></p>

<p><img align="center" src="https://github-readme-streak-stats.herokuapp.com/?user=ahmedwaleedx&" alt="ahmedwaleedx" /></p> -->
